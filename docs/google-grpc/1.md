Okay, let's break down how we can represent `pydantic-ai`'s core functionality in Elixir, focusing on the three agent setups (basic, with tools, and with a result type), and then design a clean interface for the Python-Elixir bridge.

**Understanding `pydantic-ai` Agent Types**

`pydantic-ai` agents can be categorized into these types based on their configuration:

1. **Basic Agent:**
    *   No tools.
    *   May or may not have a `result_type`.
    *   Essentially a wrapper around a prompt and an LLM call.

2. **Agent with Tools:**
    *   Has one or more tools defined using the `@agent.tool` decorator.
    *   May or may not have a `result_type`.
    *   The LLM can choose to call tools based on the user's prompt.

3. **Agent with `result_type`:**
    *   May or may not have tools.
    *   Has a defined `result_type` (a Pydantic model or `TypedDict`).
    *   The LLM is expected to return data that conforms to this type.
    *   `pydantic-ai` performs validation against the `result_type`.

**Core Functionality to Represent in Elixir:**

To integrate these agent types cleanly, we need to represent the following core `pydantic-ai` features in Elixir:

1. **Agent Definition:**
    *   Agent name (identifier).
    *   LLM model to use (e.g., "openai:gpt-4o", "gemini-1.5-pro").
    *   System prompt (can be static or dynamic).
    *   `retries` and other configuration options.

2. **Tool Definition:**
    *   Tool name.
    *   Tool description.
    *   Input parameters (name, type, description) - represented as a schema.
    *   Indication of whether the tool is implemented in Elixir or Python.

3. **Result Type Definition:**
    *   Schema representing the expected structure of the result (if `result_type` is specified).

**Proposed Elixir Representation:**

We can use Elixir structs and maps to represent these concepts.

**1. Agent Definition (`SynapseCore.Agent`):**

```elixir
defmodule SynapseCore.Agent do
  defstruct [
    name: nil,  # Required: String.t() - Unique identifier for the agent
    model: "openai:gpt-4o",  # Required: String.t() - LLM model identifier
    system_prompt: nil,  # String.t() or {:function, module(), atom()} - Static or dynamic system prompt
    tools: [],  # [Tool.t()] - List of tools available to the agent
    result_type: nil,  # Schema.t() - Schema for the expected result type (can be nil)
    retries: 1,  # integer() - Number of retries for the agent
    result_retries: nil, # integer() | nil - Number of retries for result validation
    end_strategy: :early, # :early | :last | :all - Strategy for ending a run
    python_module: nil # String.t() - Python module where the agent is defined
  ]
end
```

**2. Tool Definition (`SynapseCore.Tool`):**

```elixir
defmodule SynapseCore.Tool do
  defstruct [
    name: nil,  # Required: String.t() - Unique identifier for the tool
    description: nil,  # Required: String.t() - Description of the tool
    parameters: %{},  # Required: map() - Schema for tool parameters (e.g., using JSON Schema format)
    handler: nil  # Required: {:elixir, module(), atom()} | {:python, String.t()} - Tool implementation details
  ]
end
```

**3. Schema Representation (`SynapseCore.Schema`):**

We'll use a simplified map representation for now, similar to what we did in `schema_utils.ex`. We can enhance this later.

```elixir
# Example schema for a result type with nested objects and lists:
%{
  "type" => "object",
  "properties" => %{
    "name" => %{"type" => "string"},
    "age" => %{"type" => "integer"},
    "address" => %{
      "type" => "object",
      "properties" => %{
        "street" => %{"type" => "string"},
        "city" => %{"type" => "string"}
      }
    },
    "hobbies" => %{
      "type" => "array",
      "items" => %{"type" => "string"}
    }
  }
}
```

**Proposed Python Interface (`agent_wrapper.py`):**

We'll design the `agent_wrapper.py` API to accept agent configurations, tool definitions, and run requests in a JSON format that closely mirrors our Elixir structs.

**Example Agent Creation (POST /agents):**

**Request (from Elixir):**

```json
{
  "name": "my_agent",
  "model": "openai:gpt-4o",
  "system_prompt": "You are a helpful assistant.",
  "tools": [
    {
      "name": "get_current_date",
      "description": "Gets the current date.",
      "parameters": {
        "type": "object",
        "properties": {}
      },
      "handler": {
        "python_module": "agents.my_agent",
        "function": "get_current_date"
      }
    }
  ],
  "result_type": {
    "type": "object",
    "properties": {
      "date": {
        "type": "string",
        "format": "date"
      }
    }
  }
}
```

**Example Run Request (POST /agents/my\_agent/run):**

**Request (from Elixir):**

```json
{
    "prompt": "What's the date today?",
    "message_history": []
}
```

**Example Tool Call Request (POST /agents/my\_agent/tool\_call):**

**Request (from Elixir):**

```json
{
    "tool_name": "get_current_date",
    "args": {}
}
```

**Example Streaming Request (POST /agents/my\_agent/stream):**

**Request (from Elixir):**

```json
{
    "prompt": "Tell me a story about time travel"
}
```

**Example Streaming Response (from Python):**

```json
{"status": "chunk", "data": "Once upon a time"}
{"status": "chunk", "data": ", in a faraway land,"}
{"status": "chunk", "data": " there was a scientist..."}
# ... more chunks
{"status": "complete", "usage": {"prompt_tokens": 10, "completion_tokens": 50, "total_tokens": 60}}
```

**Example Error Response:**

```json
{
    "status": "error",
    "error_type": "ValidationError",
    "message": "Invalid result type",
    "details": [
        {
            "loc": ["date"],
            "msg": "Invalid date format",
            "type": "value_error.date"
        }
    ]
}
```

**Example Log Message:**

```json
{
    "status": "log",
    "level": "info",
    "message": "This is a log message from the Python agent."
}
```

**Minimal `agent_wrapper.py` (Conceptual):**

```python
import os
import json
import importlib
from typing import Any, AsyncIterator, Dict, List, Optional

import uvicorn
from fastapi import FastAPI, HTTPException, Request, Response, Depends
from fastapi.responses import JSONResponse, PlainTextResponse, StreamingResponse
from pydantic import BaseModel, ValidationError, Field
from pydantic_ai import Agent
from pydantic_ai.exceptions import UnexpectedModelBehavior
from pydantic_ai.message import (
    ModelMessage,
    ModelRequest,
    ModelResponse,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
)
from pydantic_ai.result import RunResult, Usage
# Assuming all agents are defined in the .agents module.
# This could be adapted to load agents from other modules.
from .agents import example_agent
# from .agents.bank_support_agent import support_agent

from .agents.example_agent import agent as example_agent
# from .agents.example_agent import agent as example_agent # , chat_agent

# Import generated gRPC stubs
# from .protos import synapse_pb2, synapse_pb2_grpc

app = FastAPI(title='Synapse Python Agent Wrapper')

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global dictionary to hold agent instances
#agent_instances: dict[str, Agent] = {"example_agent": example_agent, "bank_support_agent": support_agent}
agent_instances: Dict[str, Agent] = {}
agent_configs: Dict[str, Dict[str, Any]] = {}

# Helper functions
def _resolve_model_name(model_name: str) -> str:
    # Basic model name resolution.
    # You could add more sophisticated logic here if needed.
    return f"openai:{model_name}"

# TODO: Add this to the agent creation endpoint
# def _resolve_tools(tool_configs: List[Dict[str, Any]]) -> List[Callable]:
#     """
#     Simplified tool resolution. In a real implementation,
#     you'd likely want a more robust mechanism to map tool names
#     to Python functions, potentially using a registry or
#     dynamically loading modules.
#     """
#     tools = []
#     for config in tool_configs:
#         if config["name"] == "some_tool":
#             tools.append(some_tool)
#         # Add more tool mappings as needed
#     return tools

def _resolve_result_type(result_type_config: Dict[str, Any]) -> BaseModel:
    """
    Dynamically creates a Pydantic model from a JSON schema-like definition.
    This is a placeholder for a more complete schema translation mechanism.
    """
    fields = {}
    for field_name, field_info in result_type_config.items():
        # Assuming a simple type mapping for now
        field_type = {
            "string": str,
            "integer": int,
            "boolean": bool,
            "number": float,
            "array": list,
            "object": dict,
            "null": type(None),
        }[field_info["type"]]

        # Handle nested objects/arrays if necessary
        # ...

        fields[field_name] = (field_type, ...)  # Use ellipsis for required fields

    return create_model("ResultModel", **fields)

# Placeholder for a tool function
def some_tool(arg1: str, arg2: int) -> str:
    return f"Tool executed with {arg1} and {arg2}"




# Here's a more concise example to illustrate the concept:
# 
# from pydantic_ai import Agent
# from pydantic import BaseModel
# 
# # Define your tool function
# def my_tool(x: int, y: str) -> str:
#     """This tool takes an integer 'x' and a string 'y' and returns a string 
#     indicating the received values."""
#     return f"Received: x={x}, y={y}"
# 
# # Create an agent, passing the tool function in the `tools` list
# agent = Agent(
#     model="openai:gpt-4o",
#     system_prompt="You are a helpful assistant.",
#     tools=[my_tool],  # Register the tool here
#     result_type=BaseModel,  # You need to define a result type, even if simple
# ) 
# 
# # In essence, you register tools with pydantic-ai by passing the actual 
# # Python function objects (not just their names as strings) to the 
# # Agent constructor's tools argument.





    

@app.post("/agents")
async def create_agent(request: Request):
    """
    Creates a new agent instance.

    Expects a JSON payload like:
    {
        "agent_id": "my_agent",
        "model": "gpt-4o",
        "system_prompt": "You are a helpful assistant.",
        "tools": [
            {"name": "some_tool", "description": "Does something", "parameters": {
                "type": "object",
                "properties": {
                    "arg1": {"type": "string"},
                    "arg2": {"type": "integer"}
                }
            }}
        ],
        "result_type": {
            "type": "object",
            "properties": {
                "field1": {"type": "string"},
                "field2": {"type": "integer"}
            }
        },
        "retries": 3,
        "result_retries": 5,
        "end_strategy": "early"
    }
    """
    try:
        data = await request.json()
        agent_id = data["agent_id"]
        print(f"DATA: {data}")
        if agent_id in agent_instances:
            raise HTTPException(status_code=400, detail="Agent with this ID already exists")

        model = _resolve_model_name(data["model"])
        system_prompt = data["system_prompt"]
        tools = _resolve_tools(data.get("tools", []))
        result_type = _resolve_result_type(data.get("result_type", {}))



        # agent = Agent(
        #     model=model,
        #     system_prompt=system_prompt,
        #     tools=tools,
        #     result_type=result_type,
        #     # Add other agent parameters as needed
        # )
        agent = Agent(
            model=model,
            system_prompt=system_prompt,
            tools=[some_tool],  # Pass the tool function here
            result_type=result_type,
            # ... other agent parameters
        )

        # Dynamically import the agent module based on the provided name
        module_name = f"synapse_python.agents.{data['agent_module']}"
        agent_module = importlib.import_module(module_name)

        # Assuming each agent module has an 'agent' attribute which is an instance of pydantic_ai.Agent
        agent = agent_module.agent

        agent_instances[agent_id] = agent

        return JSONResponse({"status": "success", "agent_id": agent_id})

    except ValidationError as e:
        raise HTTPException(status_code=400, detail=e.errors())
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/agents/{agent_id}/run_sync")
async def run_agent_sync(agent_id: str, request_data: dict):
    """
    Executes an agent synchronously.

    Expects a JSON payload like:
    {
        "prompt": "What's the weather like?",
        "message_history": [],  # Optional
        "model_settings": {},  # Optional
        "usage_limits": {}  # Optional
    }
    """
    if agent_id not in agent_instances:
        raise HTTPException(status_code=404, detail="Agent not found")

    agent = agent_instances[agent_id]

    try:
        result = agent.run_sync(
            request_data["prompt"],
            message_history=request_data.get("message_history"),
            model_settings=request_data.get("model_settings"),
            usage_limits=request_data.get("usage_limits"),
            infer_name=False
        )

        # Log the successful run
        logger.info(f"Agent {agent_id} completed run_sync successfully")

        return JSONResponse(content={
            "result": to_jsonable_python(result.data),
            "usage": to_jsonable_python(result.usage),
            "messages": result.messages
        })
    except ValidationError as e:
        logger.error(f"Agent {agent_id} encountered a validation error: {e.errors()}")
        raise HTTPException(status_code=400, detail=e.errors())
    except UnexpectedModelBehavior as e:
        logger.error(f"Agent {agent_id} encountered an unexpected model behavior: {e}")
        raise HTTPException(status_code=500, detail=f"Unexpected model behavior: {e}")
    except Exception as e:
        logger.exception(f"Agent {agent_id} encountered an unexpected error: {e}")
        raise HTTPException(status_code=500, detail=str(e))



@app.post("/agents/{agent_id}/tool_call")
async def call_tool(agent_id: str, tool_name: str, request_data: dict):
    if agent_id not in agent_instances:
        raise HTTPException(status_code=404, detail="Agent not found")

    agent = agent_instances[agent_id]

    # Access the agent's tools
    agent_tools = {tool.name: tool for tool in agent.tools}

    if tool_name not in agent_tools:
        raise HTTPException(status_code=404, detail=f"Tool '{tool_name}' not found for agent '{agent_id}'")

    tool = agent_tools[tool_name]

    # Prepare the arguments for the tool function
    # Assuming the tool function expects keyword arguments
    tool_args = request_data.get("args", {})

    # Call the tool function
    try:
        # If the tool function expects a context, you need to pass it here
        # For example, if your tool function is defined like `def my_tool(ctx, **kwargs)`
        # result = tool.function(None, **tool_args)
        # Assuming no context for this example:
        result = tool.function(**tool_args)

        # Convert the result to a JSON-serializable format
        result_json = json.dumps(result)

        # Return the result as a JSON response
        return JSONResponse(content={"result": result_json})

    except Exception as e:
        logger.exception(f"Error calling tool '{tool_name}' for agent '{agent_id}': {e}")
        raise HTTPException(status_code=500, detail=f"Error calling tool: {e}")

# # Tool registry
# tool_registry: Dict[str, Callable] = {}

# def register_tool(name: str, func: Callable):
#     tool_registry[name] = func

def _resolve_tools(tool_configs: List[Dict[str, Any]]) -> List[Callable]:
    """
    Resolves tool names to their corresponding functions using a registry.
    """
    tools = []
    for config in tool_configs:
        tool_name = config["name"]
        if tool_name in tool_registry:
            tools.append(tool_registry[tool_name])
        else:
            logger.warning(f"Tool '{tool_name}' not found in registry.")
    return tools

# def _resolve_result_type(result_type_config: Dict[str, Any]) -> BaseModel:
#     """
#     Dynamically creates a Pydantic model from a JSON schema-like definition.
#     This is a placeholder for a more complete schema translation mechanism.
#     """
#     fields = {}
#     for field_name, field_info in result_type_config.items():
#         field_type = {
#             "string": str,
#             "integer": int,
#             "boolean": bool,
#             "number": float,
#             "array": list,
#             "object": dict,
#             "null": type(None),
#         }[field_info["type"]]

#         fields[field_name] = (field_type, ...)

#     return create_model("ResultModel", **fields)

# Example tool functions
# def some_tool(arg1: str, arg2: int) -> str:
#     return f"Tool executed with {arg1} and {arg2}"

# def another_tool(data: dict) -> list:
#     return list(data.values())

# # Register tools
# tool_registry: Dict[str, Callable] = {}
# tool_registry["some_tool"] = some_tool
# tool_registry["another_tool"] = another_tool

# Register tools
# tool_registry: Dict[str, Callable] = {}
# tool_registry["some_tool"] = some_tool
# tool_registry["another_tool"] = another_tool
```

**Elixir `Agent` Module:**

```elixir
# lib/synapse/agent.ex
defmodule Synapse.Agent do
  @moduledoc """
  Agent is a module that acts as a supervisor for the Agent children.
  It also defines the `child_spec/1` function which returns the specification
  for the Agent process. This is used by the Supervisor to start the Agent.
  """
  use Supervisor

  alias Synapse.Agent.Server

  @doc """
  Starts the Agent supervisor.
  """
  def start_link(opts) do
    name = opts[:name] || raise ArgumentError, "name is required"
    Supervisor.start_link(__MODULE__, opts, name: String.to_atom("#{__MODULE__}.#{name}"))
  end

  @impl true
  def init(opts) do
    children = [
      {Task.Supervisor, name: String.to_atom("Synapse.TaskSupervisor.#{opts[:name]}")},
      {Server, opts}
    ]

    Supervisor.init(children, strategy: :one_for_one)
  end

  @doc """
  Returns the child specification for the Agent process.
  """
  def child_spec(opts) do
    %{
      id: opts[:name] || raise(ArgumentError, "name is required"),
      start: {__MODULE__, :start_link, [opts]},
      type: :supervisor
    }
  end
end
```

**Elixir `SynapseCore.AgentProcess`:**

```elixir
defmodule SynapseCore.AgentProcess do
  use GenServer
  require Logger

  alias SynapseCore.{HTTPClient, JSONCodec, SchemaUtils, ToolUtils}
  alias SynapseCore.Types, as: T

  @default_timeout 60_000
  @poll_interval 500 # Interval for polling for streamed data, in milliseconds

  # ...

  @impl true
  def init(opts) do
    # ...
    {:ok, ext} = start_python_agent(state)
    # ...
    {:ok, initial_state}
  end

  # ...

  # Example of calling a tool and handling the response
  def handle_info({:tool_result, request_id, result}, state) do
    case Map.fetch(state.requests, request_id) do
      {:ok, {:run_sync, from, _}} -> # Check that the request is of type :run_sync
        # In a real scenario, you would now resume the agent's execution
        # using the result of the tool call.

        # For this example, we simply send the result back to the original caller.
        send(from, {:ok, result})

        # Remove the request from the state
        {:noreply, Map.delete(state, :requests, request_id)}

      _ ->
        Logger.error("Received tool result for unknown request ID: #{request_id}")
        {:noreply, state}
    end
  end

  # ... (other handle_info clauses)
end
```

**Further Considerations:**

*   **Streaming:** We'll need to refine the streaming implementation using `async for` on the Python side and potentially WebSockets for a more robust solution.
*   **Error Details:** The `details` field in error responses can be used to provide more granular information about validation errors or other issues.
*   **Dynamic System Prompts:** We still need to implement a mechanism for handling dynamic system prompts in Elixir.
*   **Security:** We haven't addressed security yet. We'll need to consider authentication and authorization for the API endpoints.

This detailed breakdown should give you a good starting point for implementing the core logic of the Synapse framework. Remember to focus on building a solid foundation first, and then gradually add more advanced features.
